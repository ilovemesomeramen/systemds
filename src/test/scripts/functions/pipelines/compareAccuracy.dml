#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------
##################################################################################################################
# This script will read the dirty and clean data, then it will apply the best pipeline on dirty data
# and then will classify both cleaned dataset and check if the cleaned dataset is performing same as original dataset
# in terms of classification accuracy

# Vocab = original data -> dataset without any noise, the original version with ground truths
        # cleaned data -> dirty dataset cleaned by pipeline
# read the items
# dirty dataset F
# clean dataset O
# metadata (schema and mask)
# best k pipelines and hyperparameters generated by previous script mainScript.dml

# do the initial preprocessing like dropping invalid values so that pipeline could fix them
# then recode the data to bring it into matrix format
# then construct the hyper-parameters list and call the executePipeline() on the dirty dataset
# for the comparison OHE the original dataset, there is no need to OHE the cleaned dataset because cleaning pipeline
# has a primitive for this
# Call the multilogReg on both of the datasets and compare accuracy on k=3 cross validation
######################################################################################################################

source("scripts/pipelines/scripts/utils.dml") as utils;

F = read($1, data_type="frame", format="csv", header=FALSE, 
  naStrings= ["NA", "null","  ","NaN", "nan", "", "?", "99999"]);
# read original clean data  
O = read($2, data_type="frame", format="csv", header=FALSE, 
  naStrings= ["NA", "null","  ","NaN", "nan", "", "?", "99999"]);  

metaInfo = read($3, data_type="frame", format="csv", header=FALSE);  
input = $4
pip = read(input+"pip.csv", data_type="frame", format="csv", header=FALSE);
hp = read(input+"hp.csv", data_type="matrix", format="csv", header=FALSE);

getSchema = metaInfo[1, 2:ncol(metaInfo)]
getMask = as.matrix(metaInfo[2, 2:ncol(metaInfo)])

# # 1. dropInvalid function will remove the values which are not the part 
# # of the column data type  
# print("check 1")
X = dropInvalidType(F, getSchema)

# 2. encode the categorical data
if(sum(getMask) > 0)
{
  # recode the dirty data, always recode the label
  index = utils::vectorToCsv(getMask)
  jspecR = "{ids:true, recode:["+index+"]}"
  [eX, X_meta] = transformencode(target=X, spec=jspecR);   
} 
# if no categorical value exist then just cast the frame into matrix
else
  eX = as.matrix(X)

# # 3. extract the class label  
eY = eX[, ncol(eX)]
eX = eX[, 1:ncol(eX) - 1]


# strip the mask of class label
getMask = getMask[, 1:ncol(getMask) - 1] # strip the mask of class label
getSchema = getSchema[, 1:ncol(getSchema) - 1] # strip the mask of class label


# construct hyper-parameters
ls = list();
i = 1; k = 1

# take the oversampling out from the test processing
pip1 = as.frame("")
# construct the parameter list for best hyper-parameters if the oversampling technique is part of 
# pipeline then take it out because oversampling is not applied on test dataset
# this condition is unnecessary here in this case because the input dataset is balanced and 
# instead of diving the dataset into train/test I am doing cross validations
while(k <= ncol(pip))
{
  end = as.integer(i+as.integer(as.scalar(hp[1,i])))
  mat = hp[1, i+1:end]
  i = end + 1
  if(as.scalar(pip[1,k]) != "SMOTE") {
    pip1 = cbind(pip1, pip[1,k] )
    ls = append(ls, mat)
  }
  k = k + 1
}

# # clean using best pipeline 
[cX , cY] = executePipeline(pip1[, 2:ncol(pip1)], eX, eY, getMask, ls, 1, FALSE)

if(sum(getMask) > 0)
{
  # dummycode the original data
  index = utils::vectorToCsv(getMask)
  jspec = "{ids:true, dummycode:["+index+"], recode:["+ ncol(O)+"] }"
  [oX, X_meta] = transformencode(target=O, spec=jspec); 
} 
# if no categorical value exist then just cast the frame into matrix
else 
  oX = as.matrix(O)

# # 3. extract the class label  
oY = oX[, ncol(oX)]
oX = oX[, 1:ncol(oX) - 1]



# do the k cross validations for original clean data
accuracyMatrix = crossV(oX, oY, 3, as.matrix(0), matrix("0.000001 100", rows=1, cols=2), TRUE)
accuracyMatrix = removeEmpty(target=accuracyMatrix, margin="rows")
oAcc = mean(accuracyMatrix)

# do the k cross validations for cleaned data
accuracyMatrix = crossV(cX, cY, 3, as.matrix(0), matrix("0.000001 100", rows=1, cols=2), TRUE)
accuracyMatrix = removeEmpty(target=accuracyMatrix, margin="rows")
cAcc = mean(accuracyMatrix)
tol = 1

results = (tol+cAcc) >=  oAcc
print("clean accuracy "+cAcc)
print("original accuracy "+oAcc)
write(results, $5, format = "text")


# ######################################################################
# # # Function for cross validation using hold out method
# # # Inputs: The input dataset X, Y and the value of k validation, mask of the 
# # # dataset for OHE of categorical columns, vector of ML hyper-parameters identified 
# # # via gridsearch and a boolean value of (un)weighted accuracy.
# # # Output: It return a matrix having the accuracy of each fold.
# ######################################################################

crossV = function(Matrix[double] X, Matrix[double] y, Integer k, Matrix[Double] mask,
  Matrix[Double] MLhp, Boolean isWeighted) 
return (Matrix[Double] accuracyMatrix)
{

  accuracyMatrix = matrix(0, k, 1)

  dataList = list()
  testL = list()
  data = order(target = cbind(y, X),  by = 1, decreasing=FALSE, index.return=FALSE)
  classes = table(data[, 1], 1)
  ins_per_fold = classes/k
  start_fold = matrix(1, rows=nrow(ins_per_fold), cols=1)
  fold_idxes = cbind(start_fold, ins_per_fold)

  start_i = 0; end_i = 0; idx_fold = 1;;
  for(i in 1:k)
  {
    fold_i = matrix(0, 0, ncol(data))
    start=0; end=0; 
    for(j in 1:nrow(classes))
    {
      idx = as.scalar(classes[j, 1])
      start = end + 1;
      end = end + idx
      class_j =  data[start:end, ]


      start_i = as.scalar(fold_idxes[j, 1]);
      end_i = as.scalar(fold_idxes[j, 2])

      fold_i = rbind(fold_i, class_j[start_i:end_i, ])
    }

    dataList = append(dataList, fold_i)
    fold_idxes[, 1] = fold_idxes[, 2] + 1
    fold_idxes[, 2] += ins_per_fold
    while(FALSE){}
  }

  for(i in seq(1,k))
  {
    [trainList, hold_out] = remove(dataList, i)
    trainset = rbind(trainList)
    testset = as.matrix(hold_out)
    trainX = trainset[, 2:ncol(trainset)]
    trainy = trainset[, 1]
    testX = testset[, 2:ncol(testset)]
    testy = testset[, 1]
    beta = multiLogReg(X=trainX, Y=trainy, icpt=1, reg=as.scalar(MLhp[1,1]), tol= 1e-9, 
    maxi=as.scalar(MLhp[1,2]), maxii= 50, verbose=FALSE);
    [prob, yhat, a] = multiLogRegPredict(testX, beta, testy, FALSE)
    accuracy = getAccuracy(testy, yhat, isWeighted)
    accuracyMatrix[i] = accuracy
  }

}


